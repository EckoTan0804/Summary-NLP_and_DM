{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Sense Disambiguation (WSD)\n",
    "\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/EckoTan0804/upic-repo/master/uPic/Êà™Â±è2020-05-11%2012.26.47.png\" alt=\"Êà™Â±è2020-05-11 12.26.47\" style=\"zoom:67%;\" />\n",
    "\n",
    "\n",
    "\n",
    "### Definition\n",
    "\n",
    "**Word Sense Disambiguation**\n",
    "\n",
    "- Determine which sense/meaning of a word is used in a particular context \n",
    "- Classification problem \n",
    "\n",
    "**Sense inventory**\n",
    "\n",
    "- considered senses of the words\n",
    "\n",
    "**Word Sense Discrimination**\n",
    "\n",
    "- Divide usages of a word into different meanings \n",
    "- Unsupervised Algorithms\n",
    "\n",
    "### Task\n",
    "\n",
    "**Determine which sense of a word is activated in a context** \n",
    "\n",
    "-> Find mapping $A$ for word $w_i$:\n",
    "$$\n",
    "A(i) \\subseteq \\operatorname{Sense}_{D}\\left(w_{i}\\right)\n",
    "$$\n",
    "\n",
    "- Mostly $|A(i)|=1$\n",
    "\n",
    "Model as classification problem:\n",
    "\n",
    "- Assign sense based on context and external knowledge sources \n",
    "- Every word has different number of classes\n",
    "- $n$ distinct classification tasks ($n$ Vocabulary size)\n",
    "\n",
    "#### Task-conditions\n",
    "\n",
    "- Word senses\n",
    "  - Finite set of senses for every word \n",
    "  - Automatic clustering of word senses\n",
    "\n",
    "- Sense inventories\n",
    "  - coarse-grained\n",
    "  - fine-grained\n",
    "- Text characteristics\n",
    "  - domain-oriented\n",
    "  - unrestricted\n",
    "- Target words\n",
    "  - one target word per sentence \n",
    "  - all words\n",
    "\n",
    "\n",
    "\n",
    "## Resources\n",
    "\n",
    "- **Annotated data**\n",
    "  - Input data X and output/label data Y \n",
    "  - Hard to acquire, but important \n",
    "  - Supervised training\n",
    "- **Unlabeled data**\n",
    "  - Input data X\n",
    "  - Large amounts\n",
    "  - Unsupervised data\n",
    "- **Structured resources**\n",
    "  - Thesauri\n",
    "  - Machine-readable dictionaries (MRDs)\n",
    "  - Computation lexicon (Wordnet)\n",
    "  - Ontologies\n",
    "- Unstructured resources\n",
    "  - Corpora\n",
    "  - Collocations resources\n",
    "\n",
    "\n",
    "\n",
    "## Problems\n",
    "\n",
    "- Sense definition is task dependent\n",
    "- Different algorithms for different applications\n",
    "- No discrete sense division possible\n",
    "- Knowledge acquisition bottleneck\n",
    "- Intermediate task\n",
    "\n",
    "\n",
    "\n",
    "## Application\n",
    "\n",
    "- Machine Translation (MT)\n",
    "- Information Retrieval (IR)\n",
    "- Question Answering (QA)\n",
    "- Semantic interpretation\n",
    "\n",
    "\n",
    "\n",
    "## Approaches\n",
    "\n",
    "### Dictionary- and Knowledge-Based \n",
    "\n",
    "**Lesk method / Gloss overlap**\n",
    "\n",
    "- üí° Idea: Word used together in a text are related\n",
    "\n",
    "- Method: Find word sense with the **most overlap** of dictionary definition\n",
    "\n",
    "- Input: Dictionary with definition of the different word sense\n",
    "\n",
    "- Overlap calculation\n",
    "\n",
    "  - Two words $w_1$ and $w_2$\n",
    "\n",
    "  - For each pair of senses $S_1$ in $\\operatorname{Senses}(w_1)$ and $S_2$ in $\\operatorname{Senses}(w_2)$:\n",
    "    $$\n",
    "    \\operatorname{score}\\left(S_1, S_{2}\\right)=\\left|\\operatorname{gloss}(S_1) \\cap \\operatorname{gloss}\\left(S_{2}\\right)\\right|\n",
    "    $$\n",
    "\n",
    "    - $\\operatorname{gloss}(S_1)=\\text{bag of words of definition of  } S_1$\n",
    "\n",
    "- Problem: Many words in the context -> calculation very slow ü§™\n",
    "  $$\n",
    "  \\prod_{i=1}^{n} \\operatorname{Senses}\\left(w_{i}\\right)\n",
    "  $$\n",
    "\n",
    "- Variant (simplified): Calculate overlap between context (set of words in surrounding sentence or paragraph) and gloss:\n",
    "  $$\n",
    "  \\operatorname{score}(S)=|\\operatorname{context}(w) \\cap \\operatorname{gloss}(S)|\n",
    "  $$\n",
    "\n",
    "  - Example:\n",
    "\n",
    "    <img src=\"https://raw.githubusercontent.com/EckoTan0804/upic-repo/master/uPic/Êà™Â±è2020-05-11%2021.56.13.png\" alt=\"Êà™Â±è2020-05-11 21.56.13\" style=\"zoom:67%;\" />\n",
    "\n",
    "  - Problems:\n",
    "    - depend heavily on the exact definition \n",
    "    - definitions are often very short\n",
    "\n",
    "### Supervised \n",
    "\n",
    "üí° Train classifier using annotated examples (i.e., annotated text corpora)\n",
    "\n",
    "- Input features: Use context to disambiguate words\n",
    "- Problems:\n",
    "  - high-dimension of the feature space \n",
    "  - data sparseness problem\n",
    "- Techniques:\n",
    "  - Naive Bayes classifier \n",
    "  - Instance-based Learning\n",
    "  - SVM\n",
    "  - Ensemble Methods\n",
    "  - Neural Networks (e.g. Bi-LSTM)\n",
    "\n",
    "#### Feature extraction\n",
    "\n",
    "**Feature vector**:\n",
    "\n",
    "- Vector describing input data \n",
    "- Fixed number of dimensions\n",
    "  - Challenges:\n",
    "    - Variable sentence length\n",
    "    - Unknown number of words\n",
    "\n",
    "**Two kinds of features in the vectors:**\n",
    "\n",
    "- **Collocational**: Features about words at **specific** positions near target word\n",
    "\n",
    "  - Think as a list\n",
    "\n",
    "  - Often limited to just word identity and POS\n",
    "\n",
    "  - Example:\n",
    "\n",
    "    <img src=\"https://raw.githubusercontent.com/EckoTan0804/upic-repo/master/uPic/Êà™Â±è2020-05-11%2023.04.45.png\" alt=\"Êà™Â±è2020-05-11 23.04.45\" style=\"zoom:50%;\" />\n",
    "\n",
    "- Bag-of-words: Features abt words that occur anywhere in the window (regardless of position)\n",
    "\n",
    "  - Think as \"an unordered set of words\"\n",
    "\n",
    "  - Typically limited to frequency counts\n",
    "\n",
    "  - How it works?\n",
    "\n",
    "    - Counts of words occur within the window.\n",
    "    - First choose a vocabulary\n",
    "    - Then count how often each of those terms occurs in a given window\n",
    "      - sometimes just a binary ‚Äúindicator‚Äù 1 or 0\n",
    "\n",
    "  - Example:\n",
    "\n",
    "    <img src=\"https://raw.githubusercontent.com/EckoTan0804/upic-repo/master/uPic/Êà™Â±è2020-05-11%2023.06.20.png\" alt=\"Êà™Â±è2020-05-11 23.06.20\" style=\"zoom:50%;\" />\n",
    "\n",
    "**Text processing**\n",
    "\n",
    "- Tokenization\n",
    "- Part-of-speech tagging\n",
    "- Lemmatization\n",
    "- Chunking: divided text into syntactically correlated part\n",
    "- Parsing\n",
    "\n",
    "**Feature definition**\n",
    "\n",
    "- Local features\n",
    "  - surrounding words, POS tags, position with respect to target word\n",
    "- Topical/Global features\n",
    "  - general topic of a text\n",
    "  - mostly bag-of-words representation of (sentence, paragraph, ...)\n",
    "- Syntactic features\n",
    "  - syntactic clues\n",
    "  - can be outside the local context\n",
    "- Semantic features\n",
    "  - previous determined sense of words in the context\n",
    "\n",
    "#### Naive Bayes classifier\n",
    "\n",
    "- Input: \n",
    "\n",
    "  - a word $w$ *in a text window* $d$ *(which we‚Äôll call a ‚Äúdocument‚Äù)*\n",
    "\n",
    "  - a fixed set of classes $C = \\{c_1, c_2, \\dots, c_j\\}$\n",
    "\n",
    "  - A training set of $m$ hand-labeled text windows again called\n",
    "\n",
    "    ‚Äúdocuments‚Äù $(d_1, c_1), \\dots, (d_m, c_m)$\n",
    "\n",
    "- Output: a learn classifier $\\gamma: d \\to c$\n",
    "\n",
    "- $P(c)$: prior probability of that sense\n",
    "\n",
    "  - Counting in a labeled training set\n",
    "\n",
    "- $P(w|c)$: conditional probability of a word given a particular sense\n",
    "\n",
    "  - $p(w|c) = \\frac{\\operatorname{count}(w, c)}{\\operatorname{count}(c)}$\n",
    "\n",
    "  (We get both of these from a tagged corpus)\n",
    "\n",
    "- Example:\n",
    "\n",
    "  <img src=\"https://raw.githubusercontent.com/EckoTan0804/upic-repo/master/uPic/Êà™Â±è2020-05-11%2022.45.57.png\" alt=\"Êà™Â±è2020-05-11 22.45.57\" style=\"zoom:67%;\" />\n",
    "\n",
    "#### Instance-based Learning\n",
    "\n",
    "- Build classification model based on examples\n",
    "\n",
    "- k-Nearest Neighbor (kNN) algorithm\n",
    "\n",
    "- üí°Idea:\n",
    "\n",
    "  - represent examples in vector space\n",
    "  - define distance metric in vector space\n",
    "  - find k nearest neighbor\n",
    "  - take most common sense in the k nearest neighbors\n",
    "\n",
    "- Distance: e.g., Hamming distance\n",
    "  $$\n",
    "  \\Delta\\left(x, x_{i}\\right)=\\sum w_{j} \\delta\\left(x_{j}, x_{i_{j}}\\right)\n",
    "  $$\n",
    "\n",
    "  - $\\delta\\left(x_{j}, x_{i_j}\\right)=0$ if $x_{j}=x_{i_j},$ else 1\n",
    "  - $w_j$: weight (e.g., Gain ration measure)\n",
    "\n",
    "  > In [information theory](https://en.wikipedia.org/wiki/Information_theory), the **Hamming distance** between two [strings](https://en.wikipedia.org/wiki/String_(computer_science)) of equal length is the number of positions at which the corresponding [symbols](https://en.wikipedia.org/wiki/Symbol) are different. In other words, it measures the minimum number of *substitutions* required to change one string into the other, or the minimum number of *errors* that could have transformed one string into the other. \n",
    "  >\n",
    "  > Example:\n",
    "  >\n",
    "  > ![Êà™Â±è2020-05-12 11.29.21](https://raw.githubusercontent.com/EckoTan0804/upic-repo/master/uPic/Êà™Â±è2020-05-12%2011.29.21.png)\n",
    "\n",
    "#### Ensemble Methods\n",
    "\n",
    "Combine different classifier\n",
    "\n",
    "- classifier have strength in different situ\tation \n",
    "- improve by asking several experts\n",
    "\n",
    "Algorithm:\n",
    "\n",
    "- Score input by several First-order classifier \n",
    "- Combine results\n",
    "\n",
    "- Result:\n",
    "\n",
    "  - Only best hypothesis (majority vote)\n",
    "\n",
    "    - take decision of most classifiers\n",
    "\n",
    "    - if tie, randomly choose between them\n",
    "      $$\n",
    "      \\hat{S}=\\underset{S_i \\in \\operatorname{Sense}_D(w)}{\\operatorname{argmax}}\\left|\\left\\{j: \\operatorname{vote}\\left(C_{j}\\right)=S_{j} |\\right.\\right.\n",
    "      $$\n",
    "\n",
    "  - Score for all hypothesis (Probability Mixture)\n",
    "\n",
    "    - Normalize scores of every classifier to get probability\n",
    "      $$\n",
    "      P_{C_{j}}(S_i)=\\frac{\\operatorname{score}\\left(C_{j}, S_i\\right)}{\\sum \\operatorname{score}\\left(C_{j}, S_i\\right)}\n",
    "      $$\n",
    "\n",
    "    - Take class with *highest* sum of probabilities\n",
    "\n",
    "    $$\n",
    "    \\hat{S}=\\underset{S_i \\in \\operatorname{Sense}_D(w)}{\\operatorname{argmax}}\\sum_{j=1}^{m}P_{c_j}(S_i)\n",
    "    $$\n",
    "\n",
    "  - Ranking of all hypothesis (Rank-based Combination)\n",
    "    $$\n",
    "    \\hat{S}=\\underset{S_i \\in \\operatorname{Sense}_D(w)}{\\operatorname{argmax}}\\sum_{j=1}^{m} -\\operatorname{Rank}_{c_j}(S_i)\n",
    "    $$\n",
    "\n",
    "### Semi-supervised\n",
    "\n",
    "‚ÄºÔ∏è **Knowledge acquisition bottleneck**: hard to get large amounts of annotated data\n",
    "\n",
    "üí° **Idea** of Semi-supervised approaches:\n",
    "\n",
    "- Some initial model trained on small amounts of annotated data \n",
    "- Improve model using raw data\n",
    "\n",
    "**Bootstrapping**:\n",
    "\n",
    "- Seed data:\n",
    "\n",
    "  - manual annotated\n",
    "  - surefire decision rules\n",
    "\n",
    "- Train classifier on annotated data A \n",
    "\n",
    "- Select subset U‚Äô of unlabeled data \n",
    "\n",
    "- Annotate U‚Äô with classifier\n",
    "\n",
    "- Filter most reliable examples\n",
    "\n",
    "- Add examples to A\n",
    "\n",
    "- Repeat from training\n",
    "\n",
    "  ![Êà™Â±è2020-05-12 11.39.03](https://raw.githubusercontent.com/EckoTan0804/upic-repo/master/uPic/Êà™Â±è2020-05-12%2011.39.03.png)\n",
    "\n",
    "**Self-training**: \n",
    "\n",
    "- always use same classifier\n",
    "\n",
    "**Co-training**:\n",
    "\n",
    "- train classifier 1 (e.g. using local feature)\n",
    "- Annotate $P‚Äô$ with classifier 1\n",
    "- train classifier 2 (e.g. topical information) on $P‚Äô$ and A\n",
    "- Annotate $P‚Äô_2$ with classifier 2\n",
    "-  train classifier 1 ...\n",
    "\n",
    "### Unsupervised\n",
    "\n",
    "üí° **Idea**: \n",
    "\n",
    "- If a word is used in similar context, the meaning should be similar\n",
    "- If the word is used in completely different context, different meaning\n",
    "\n",
    "**Approach**: Cluster contexts of words\n",
    "\n",
    "#### Context clustering\n",
    "\n",
    "**Word space model**:\n",
    "\n",
    "- Vector space with dimension of the words\n",
    "\n",
    "- vector for word $w$:\n",
    "\n",
    "  - $j$-th component: number of co-occurs of $w$ and $w_j$\n",
    "\n",
    "- **Similarity**:\n",
    "  $$\n",
    "  \\operatorname{sim}(v, w)=\\frac{v^{*} w}{|v|^{*}|w|}=\\frac{\\displaystyle\\sum_{i=1}^{m} v_{i} * w_{i}}{\\sqrt{\\displaystyle\\sum_{i=1}^{m} v_{i}^{2} \\displaystyle\\sum_{i=1}^{m} w_{i}^{2}}}\n",
    "  $$\n",
    "\n",
    "- Example:\n",
    "\n",
    "  - Dimension: (food, bank)\n",
    "  - restaurant=(210, 80) \n",
    "  - money = (100, 250)\n",
    "\n",
    "  <img src=\"https://raw.githubusercontent.com/EckoTan0804/upic-repo/master/uPic/Êà™Â±è2020-05-12%2011.58.38.png\" alt=\"Êà™Â±è2020-05-12 11.58.38\" style=\"zoom:67%;\" />\n",
    "\n",
    "- ‚ÄºÔ∏è Problem:\n",
    "\n",
    "  - sparse representation\n",
    "  - latent semantic analyses (LSA)\n",
    "\n",
    "**Context representation**\n",
    "\n",
    "- Second-order vectors: *average* of all word vectors in the context\n",
    "\n",
    "- Example:\n",
    "\n",
    "  <img src=\"https://raw.githubusercontent.com/EckoTan0804/upic-repo/master/uPic/Êà™Â±è2020-05-12%2012.02.09.png\" alt=\"Êà™Â±è2020-05-12 12.02.09\" style=\"zoom:67%;\" />\n",
    "\n",
    "**Cluster contexts**\n",
    "\n",
    "- Agglomerative clustering\n",
    "  - Start with one context per cluster \n",
    "  - Merge most similar clusters \n",
    "  - Continue until threshold is reached\n",
    "\n",
    "**Co-occurrence Graphs**\n",
    "\n",
    "- HyperLex: Co-occurrence graph for one target ambiguous word $w$\n",
    "\n",
    "  - Nodes: All Words occurring in a paragraph with $w$\n",
    "\n",
    "  - Edge: words occur in same paragraph\n",
    "\n",
    "  - Weight:\n",
    "    $$\n",
    "    \\begin{array}{c}\n",
    "    w_{i j}=1-\\max \\left(P\\left(w_{i} | w_{j}\\right), P\\left(w_{j} | w_{i}\\right)\\right) \\\\\n",
    "    P\\left(w_{i} | w_{j}\\right)=\\frac{f r e q_{i j}}{f r e q_{j}}\n",
    "    \\end{array}\n",
    "    $$\n",
    "\n",
    "    - Low weight -> High probability of co-occurring\n",
    "\n",
    "    - Discard edges with very high weight\n",
    "\n",
    "      <img src=\"https://raw.githubusercontent.com/EckoTan0804/upic-repo/master/uPic/Êà™Â±è2020-05-12%2012.20.11.png\" alt=\"Êà™Â±è2020-05-12 12.20.11\" style=\"zoom:67%;\" />\n",
    "\n",
    "  - How HyperLex works?\n",
    "\n",
    "    - Select Hubs (Nodes with highest degree) \n",
    "\n",
    "    - Connect target words with weight 0 to hubs \n",
    "\n",
    "    - Calculate Minimal Spanning Tree\n",
    "\n",
    "    - See Target word in Context $W = (w_1, w_2, \\dots, w_n)$\n",
    "\n",
    "    - Calculate vector for every word with $s_k$ (if $w_j$ ancestor of $h_k$)\n",
    "      $$\n",
    "      s_{k}=\\frac{1}{1+d\\left(h_{k}, w_{j}\\right)}\n",
    "      $$\n",
    "\n",
    "    - Sum all vectors and assign to hub with highest sum\n",
    "\n",
    "      <img src=\"https://raw.githubusercontent.com/EckoTan0804/upic-repo/master/uPic/Êà™Â±è2020-05-12%2012.23.12.png\" alt=\"Êà™Â±è2020-05-12 12.23.12\" style=\"zoom:67%;\" />\n",
    "\n",
    "#### Evaluation\n",
    "\n",
    "- **Hand-annotated data** \n",
    "\n",
    "  - Precision\n",
    "  - Recall\n",
    "\n",
    "- **Task**:\n",
    "\n",
    "  - Lexical sample: only some words need to be disambiguate \n",
    "  - All-words: all words need to be disambiguate\n",
    "\n",
    "- **Baseline**:\n",
    "\n",
    "  - Random baseline: Randomly choose one class\n",
    "\n",
    "  - First Sense Baseline: Always take most common sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
