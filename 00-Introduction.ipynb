{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to NLP\n",
    "\n",
    "## What is NLP?\n",
    "\n",
    "> Wikipedia:\n",
    "> **Natural language processing** (**NLP**) is a field of computer science, artificial intelligence, and linguistics concerned with the interactions between computers and human (natural) languages. As such, NLP is related to the area of human‚Äìcomputer interaction.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/EckoTan0804/upic-repo/master/uPic/Êà™Â±è2020-05-10%2023.56.28.png\" alt=\"Êà™Â±è2020-05-10 23.56.28\" style=\"zoom: 67%;\" />\n",
    "\n",
    "\n",
    "\n",
    "## What is Dialog Modeling\n",
    "\n",
    "- Designing/building a spoken dialog system with its goals, user handling etc.\n",
    "\n",
    "- Synonymous to dialog management (DM)\n",
    "\n",
    "  \n",
    "\n",
    "  <img src=\"https://raw.githubusercontent.com/EckoTan0804/upic-repo/master/uPic/Êà™Â±è2020-05-10%2023.58.28.png\" alt=\"Êà™Â±è2020-05-10 23.58.28\" style=\"zoom:50%;\" />\n",
    "\n",
    "- Examples\n",
    "\n",
    "  - Goal-oriented dialog\n",
    "\n",
    "  - Social dialog / Chat bot\n",
    "\n",
    "    \n",
    "\n",
    "## How to do NLP?\n",
    "\n",
    "- Aim: Understand **linguistic** structure of communication\n",
    "- Idea: There are rules to decide if a sentence is correct or not \n",
    "  - A proper sentence needs to have:\n",
    "    - 1 Subject\n",
    "    - 1 Verb\n",
    "    - several objects (depending on the verb's valence)\n",
    "\n",
    "### TL;DR\n",
    "\n",
    "- Task:\n",
    "  - Linguistic dimension: Syntax, semantics, pragmatics\n",
    "  - Level: Word, word groups, sentence, beyond sentences\n",
    "- Approaches\n",
    "  - Technique: \n",
    "    - Rule-based, \n",
    "    - Statistical, \n",
    "    - Neural\n",
    "  -  Learning scenario: \n",
    "    - Supervised, \n",
    "    - semi-supervised, \n",
    "    - unsupervised, \n",
    "    - reinforcement learning\n",
    "  - Model:\n",
    "    - Classification, \n",
    "    - sequence classification, \n",
    "    - sequence labeling, \n",
    "    - sequence to sequence, \n",
    "    - structure prediction\n",
    "\n",
    "### Technique\n",
    "\n",
    "#### Hand-written rules to parse the sentences (Rule-based)\n",
    "\n",
    "‚ÄºÔ∏èProblems \n",
    "\n",
    "- There is no fixed set of rules\n",
    "- Language changes over time\n",
    "- A(ny?) language is constantly influenced by other languages\n",
    "- Classification of words into POS tags not always clear\n",
    "\n",
    "#### **Corpus-based Approaches to NLP** (Statistical)\n",
    "\n",
    "- **Corpus = large collection of *annotated* texts (or speech files)**\n",
    "- üëç **advantages**:\n",
    "  - Automatically learn rules from data \n",
    "  - Statistical Models ‚Üí no hard decision \n",
    "  - Use machine learning approaches\n",
    "    - Possible since larger computation resources \n",
    "  - Corpus will concentrate on most common approaches\n",
    "- **Input**:\n",
    "  - Data (Text corpora) \n",
    "  - Machine learning algorithm\n",
    "- **Output**: Statistical model\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/EckoTan0804/upic-repo/master/uPic/Êà™Â±è2020-05-11%2000.43.05.png\" alt=\"Êà™Â±è2020-05-11 00.43.05\" style=\"zoom: 67%;\" />\n",
    "\n",
    "\n",
    "\n",
    "- **Problems of simple statistical models**: feature engineering \n",
    "  - What features are important to determine the POS tag\n",
    "    - Word ending\n",
    "    -  Surrounding words\n",
    "    - Capitalization \n",
    "\n",
    "#### **Deep learning Approaches to NLP** (Neural)\n",
    "\n",
    "- Use neural networks to automatically infer features \n",
    "- Better generalization\n",
    "- Successfully applied to many NLP tasks\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/EckoTan0804/upic-repo/master/uPic/Êà™Â±è2020-05-11%2000.45.08.png\" alt=\"Êà™Â±è2020-05-11 00.45.08\" style=\"zoom:67%;\" /> \n",
    "\n",
    "### Learning scenarios\n",
    "\n",
    "- Supervised learning\n",
    "- Unsupervised learning\n",
    "- Semi supervised learning\n",
    "- Reinforcement learning\n",
    "\n",
    "### Model types\n",
    "\n",
    "| Model type                 | Input                                                        | Output                              | Example task              |\n",
    "| -------------------------- | ------------------------------------------------------------ | ----------------------------------- | ------------------------- |\n",
    "| Classification             | **Fix** input size <br />*(E.g. word and surrounding k words)* | Label                               | Word sense disambiguation |\n",
    "| Sequence classification    | Sequence with **variable** length                            | Label                               | Sentiment analysis        |\n",
    "| Sequence labelling         | Sequence with **variable** length                            | Label sequence with **same** length | Named entity recognition  |\n",
    "| Sequence to Sequence model | Sequence with **variable** length                            | Sequence **variable** length        | Summarization             |\n",
    "| Structure prediction       | Sequence with **variable** length                            | Complex structure                   | Parsing                   |\n",
    "\n",
    "### Resources\n",
    "\n",
    "- Texts\n",
    "  - Brown Corpus\n",
    "  - Penn Treebank\n",
    "  - Europarl\n",
    "  - Google books corpus\n",
    "- Dictionaries/Ontologies\n",
    "  - WordNet, \n",
    "  - GermaNet, \n",
    "  - EuroWordNet\n",
    "\n",
    "### Approaches to Dialog Modeling\n",
    "\n",
    "- Many problems of NLP also apply to Dialog Modeling\n",
    "- Use conversational corpora for learning interaction patterns\n",
    "  - Meeting Corpus (multiparty conversation)\n",
    "  - Switchboard Corpus (telephone speech)\n",
    "- Problems ‚ÄºÔ∏è\n",
    "  - Very domain dependent\n",
    "  - Need human interaction in training\n",
    "\n",
    "\n",
    "\n",
    "## Why is NLP hard?\n",
    "\n",
    " **Ambiguities! Ambiguities! Ambiguities!**\n",
    "\n",
    "### Ambiguities\n",
    "\n",
    "Examples:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/EckoTan0804/upic-repo/master/uPic/Êà™Â±è2020-05-11%2011.19.29.png\" alt=\"Êà™Â±è2020-05-11 11.19.29\" style=\"zoom:67%;\" />\n",
    "\n",
    "### Rare events\n",
    "\n",
    "- Calculate probabilities for events/words \n",
    "\n",
    "- Most words occur only very rarely\n",
    "  - Most words occur one time\n",
    "  - What to do with words that occur not in training data? üßê\n",
    "\n",
    "**Zipf's Law**\n",
    "$$\n",
    "f \\propto \\frac{1}{r}\n",
    "$$\n",
    "\n",
    "- order list of words by occurrence\n",
    "- rank: position in the list\n",
    "\n",
    "> The frequency of any word is [inversely proportional](https://en.wikipedia.org/wiki/Inversely_proportional) to its rank in the [frequency table](https://en.wikipedia.org/wiki/Frequency_table).\n",
    ">\n",
    "> Thus the most frequent word will occur approximately twice as often as the second most frequent word, three times as often as the third most frequent word\n",
    ">\n",
    "> For example, in the [Brown Corpus](https://en.wikipedia.org/wiki/Brown_Corpus) of American English text, the word *[the](https://en.wikipedia.org/wiki/English_articles#Definite_article)* is the most frequently occurring word, and by itself accounts for nearly 7% of all word occurrences (69,971 out of slightly over 1 million). True to Zipf's Law, the second-place word *of* accounts for slightly over 3.5% of words (36,411 occurrences), followed by *and* (28,852). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
